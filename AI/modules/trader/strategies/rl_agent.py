# AI/modules/trader/strategies/rl_agent.py
"""
[RL ê¸°ë°˜ ì „ëžµ]
- í•™ìŠµëœ PPO ëª¨ë¸ íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ í–‰ë™ì„ ê²°ì •í•©ë‹ˆë‹¤.
- ë°±í…ŒìŠ¤íŠ¸(run_portfolio.py)ë‚˜ ì‹¤ì „ ë§¤ë§¤ì—ì„œ ì´ í´ëž˜ìŠ¤ë¥¼ í˜¸ì¶œí•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.
"""

import os
import numpy as np
from stable_baselines3 import PPO

class RLAgentStrategy:
    def __init__(self, model_path=None):
        if model_path is None:
            # ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
            current_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.abspath(os.path.join(current_dir, "../../../.."))
            model_path = os.path.join(project_root, "AI/data/weights/rl_agent_ppo.zip")
            
        if os.path.exists(model_path):
            self.model = PPO.load(model_path)
            print(f"ðŸ¤– RL ì—ì´ì „íŠ¸ ë¡œë“œ ì„±ê³µ: {model_path}")
        else:
            print(f"âš ï¸ RL ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {model_path} (ëžœë¤ ëª¨ë“œë¡œ ë™ìž‘)")
            self.model = None

    def get_action(self, obs_vector: np.ndarray) -> dict:
        """
        Args:
            obs_vector (np.array): [í˜„ê¸ˆë¹„ìœ¨, ìˆ˜ìµë¥ , RSI, MACD, ë³€ë™ì„±, ê±°ëž˜ëŸ‰]
        Returns:
            dict: {'type': 'BUY'/'SELL', 'amount': float}
        """
        if self.model:
            action, _ = self.model.predict(obs_vector, deterministic=True)
            val = float(action[0])
        else:
            val = np.random.uniform(-1, 1) # ëª¨ë¸ ì—†ìœ¼ë©´ ëžœë¤

        # í–‰ë™ í•´ì„ (rl_env.pyì˜ ë¡œì§ê³¼ ë™ì¼í•´ì•¼ í•¨)
        if val > 0.05:
            return {'type': 'BUY', 'amount': val}
        elif val < -0.05:
            return {'type': 'SELL', 'amount': abs(val)}
        else:
            return {'type': 'HOLD', 'amount': 0}