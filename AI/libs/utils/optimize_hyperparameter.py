import os
import sys
import numpy as np
import pandas as pd
import tensorflow as tf
import optuna
from sklearn.metrics import accuracy_score
from tensorflow.keras.callbacks import EarlyStopping

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  ê²½ë¡œ ë° ëª¨ë“ˆ ë¡œë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_this_file = os.path.abspath(__file__)
project_root = os.path.dirname(os.path.dirname(_this_file)) 
if project_root not in sys.path: sys.path.append(project_root)

from modules.models import build_transformer_classifier
from modules.features import FEATURES, build_features
from training.train_transformer import (
    _fetch_db_ohlcv_for_tickers, 
    load_all_tickers_from_db,
    _label_by_future_return,
    _build_sequences,
    _align_labels
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  ë°ì´í„° ë¡œë“œ (ì „ì—­ ë³€ìˆ˜ë¡œ í•œ ë²ˆë§Œ ë¡œë“œí•˜ì—¬ ì†ë„ í–¥ìƒ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("[AutoML] ë°ì´í„° ë©”ëª¨ë¦¬ ë¡œë”© ì¤‘...")
tickers = load_all_tickers_from_db(verbose=False)
raw_df = _fetch_db_ohlcv_for_tickers(tickers, "2023-01-01", "2024-12-31") # ê¸°ê°„ ì„¤ì •

# ì „ì²˜ë¦¬ ë¯¸ë¦¬ ìˆ˜í–‰ (í”¼ì²˜ ìƒì„±ê¹Œì§€)
grouped_data = []
for t, g in raw_df.groupby("ticker"):
    g = g.sort_values('ts_local').set_index('ts_local')
    feats = build_features(g)
    # NaN ì œê±°ëŠ” ë‚˜ì¤‘ì— ì‹œí€€ìŠ¤ ë§Œë“¤ ë•Œ ì²˜ë¦¬
    grouped_data.append(feats)

print(f"[AutoML] {len(grouped_data)}ê°œ ì¢…ëª© ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  Objective Function (Optunaê°€ ìµœì í™”í•  ëŒ€ìƒ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def objective(trial):
    # 1. íƒìƒ‰í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜ (Search Space)
    SEQ_LEN = trial.suggest_categorical('seq_len', [30, 60, 128]) # ìœˆë„ìš° í¬ê¸°
    PRED_H = 7  # ì˜ˆì¸¡ ê¸°ê°„ì€ ê³ ì • (í•„ìš” ì‹œ ë³€ê²½ ê°€ëŠ¥)
    
    # â˜… í•µì‹¬: ë§¤ìˆ˜/ë§¤ë„ ê¸°ì¤€ ì„ê³„ê°’ë„ ìµœì í™” ëŒ€ìƒì— í¬í•¨
    HOLD_THR = trial.suggest_float('hold_thr', 0.002, 0.02) # 0.2% ~ 2.0% ì‚¬ì´ íƒìƒ‰
    
    # ëª¨ë¸ ê´€ë ¨ íŒŒë¼ë¯¸í„°
    LEARNING_RATE = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)
    DROPOUT = trial.suggest_float('dropout', 0.1, 0.5)
    NUM_LAYERS = trial.suggest_int('num_layers', 1, 4)
    D_MODEL = trial.suggest_categorical('d_model', [64, 128, 256])
    
    # 2. ë°ì´í„°ì…‹ êµ¬ì„± (íŒŒë¼ë¯¸í„°ì— ë”°ë¼ ë¼ë²¨ì´ ë°”ë€Œë¯€ë¡œ ë§¤ë²ˆ ìƒì„±í•´ì•¼ í•¨)
    X_all, y_all, r_all = [], [], []
    model_feats = [f for f in FEATURES if f != "CLOSE_RAW"]
    
    for feats in grouped_data:
        # ë¼ë²¨ë§ ë‹¤ì‹œ ìˆ˜í–‰ (HOLD_THRê°€ ë°”ë€Œë¯€ë¡œ)
        labels = _label_by_future_return(feats["CLOSE_RAW"], PRED_H, HOLD_THR)
        future_ret = (feats["CLOSE_RAW"].shift(-PRED_H) / feats["CLOSE_RAW"]) - 1.0
        
        valid = feats.notna().all(axis=1) & labels.notna() & future_ret.notna()
        f_valid = feats[valid]
        l_valid = labels[valid]
        r_valid = future_ret[valid]
        
        if len(f_valid) < SEQ_LEN: continue
            
        X_seq = _build_sequences(f_valid, model_feats, SEQ_LEN)
        y_seq = _align_labels(f_valid, l_valid, SEQ_LEN)
        r_seq = _align_labels(f_valid, r_valid, SEQ_LEN) # ìˆ˜ìµë¥  ì‹œí€€ìŠ¤
        
        # ê¸¸ì´ ë§ì¶¤
        min_len = min(len(X_seq), len(y_seq), len(r_seq))
        X_all.append(X_seq[:min_len])
        y_all.append(y_seq[:min_len])
        r_all.append(r_seq[:min_len])

    if not X_all:
        return -999.0 # ì‹¤íŒ¨ ì‹œ ë§¤ìš° ë‚®ì€ ì ìˆ˜

    X = np.concatenate(X_all, axis=0)
    y = np.concatenate(y_all, axis=0).astype(int)
    r = np.concatenate(r_all, axis=0)
    
    # Train/Val ë¶„ë¦¬ (ìµœê·¼ 20%ë¥¼ ê²€ì¦ìš©ìœ¼ë¡œ ì‚¬ìš©)
    split_idx = int(len(X) * 0.8)
    X_train, X_val = X[:split_idx], X[split_idx:]
    y_train, y_val = y[:split_idx], y[split_idx:]
    r_val = r[split_idx:] # ê²€ì¦ ë°ì´í„°ì˜ ì‹¤ì œ ìˆ˜ìµë¥ 
    
    # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (HOLD í¸í–¥ ë°©ì§€)
    from sklearn.utils.class_weight import compute_class_weight
    classes = np.unique(y_train)
    weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)
    class_weight_dict = {i: w for i, w in zip(classes, weights)}

    # 3. ëª¨ë¸ ë¹Œë“œ ë° í•™ìŠµ
    # (build_transformer_classifier í•¨ìˆ˜ íŒŒë¼ë¯¸í„°ë¥¼ ìˆ˜ì •í•˜ì—¬ dropout ë“±ì„ ë°›ì„ ìˆ˜ ìˆê²Œ í•´ì•¼ í•¨. 
    #  ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¡œ optimizer ì„¤ì •ë§Œ ë³´ì—¬ì¤Œ)
    model = build_transformer_classifier(SEQ_LEN, X.shape[2]) 
    
    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)
    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    
    # ì†ë„ë¥¼ ìœ„í•´ EpochëŠ” ì§§ê²Œ ì„¤ì • (Pruning í™œìš© ê°€ëŠ¥)
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=5,  # ìµœì í™” ë‹¨ê³„ì—ì„œëŠ” epochë¥¼ ì¤„ì„
        batch_size=512,
        class_weight=class_weight_dict,
        verbose=0, # ë¡œê·¸ ìˆ¨ê¹€
        callbacks=[EarlyStopping(patience=2)]
    )
    
    # 4. ë°±í…ŒìŠ¤íŒ… ì‹œë®¬ë ˆì´ì…˜ (ê²€ì¦ ë°ì´í„°ì…‹ ëŒ€ìƒ)
    y_pred_probs = model.predict(X_val, batch_size=512, verbose=0)
    y_pred = np.argmax(y_pred_probs, axis=1)
    
    # BUY ì‹ í˜¸(0)ì— ëŒ€í•œ ìˆ˜ìµë¥  ê³„ì‚°
    buy_signals = (y_pred == 0)
    
    if np.sum(buy_signals) == 0:
        return 0.0 # ë§¤ìˆ˜ ì‹ í˜¸ê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ 0ì  ì²˜ë¦¬
        
    # í‰ê°€ì§€í‘œ: ì´ ìˆ˜ìµë¥  (Total Return) ë˜ëŠ” ìƒ¤í”„ ì§€ìˆ˜
    # ì—¬ê¸°ì„œëŠ” 'BUY ì‹ í˜¸ ì‹œ í‰ê·  ìˆ˜ìµë¥  * ì ì¤‘ë¥ 'ì„ ì ìˆ˜ë¡œ ì‚¬ìš©í•´ ë´…ë‹ˆë‹¤.
    avg_return = np.mean(r_val[buy_signals])
    win_rate = np.mean(r_val[buy_signals] > 0)
    
    # ì ìˆ˜ ì‚°ì • ê³µì‹ (ì‚¬ìš©ì ì •ì˜ ê°€ëŠ¥)
    # ì˜ˆ: í‰ê·  ìˆ˜ìµë¥ ì´ ë†’ìœ¼ë©´ì„œë„, ë„ˆë¬´ ì ê²Œ ê±°ë˜í•˜ì§€ ì•ŠëŠ” ê· í˜•ì  ì°¾ê¸°
    score = avg_return * 100 
    
    # ë„ˆë¬´ ìœ„í—˜í•œ ë§¤ë§¤ë¥¼ ë§‰ê¸° ìœ„í•´ ìŠ¹ë¥  í˜ë„í‹° ì¶”ê°€ ê°€ëŠ¥
    if win_rate < 0.5: 
        score *= 0.5 
        
    return score

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  ì‹¤í–‰ (Main)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    # ë°©í–¥: maximize (ìˆ˜ìµë¥  ìµœëŒ€í™”)
    study = optuna.create_study(direction="maximize")
    
    print("ğŸš€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘ (ì´ 20íšŒ ì‹œë„)")
    study.optimize(objective, n_trials=20)
    
    print("\n" + "="*50)
    print("ğŸ† Best Trial ê²°ê³¼:")
    print(f"  Value (Score): {study.best_value}")
    print("  Params:")
    for key, value in study.best_params.items():
        print(f"    {key}: {value}")
    print("="*50)
    
    # ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥ (íŒŒì¼ ë“±)
    # ...