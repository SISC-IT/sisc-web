# AI/modules/trader/train/train_ppo.py
"""
[PPO í•™ìŠµ ì‹¤í–‰ê¸°]
- rl_env.py í™˜ê²½ì„ ë¶ˆëŸ¬ì™€ AIë¥¼ í›ˆë ¨ì‹œí‚µë‹ˆë‹¤.
- í•™ìŠµëœ ëª¨ë¸(.zip)ì„ 'AI/data/weights/rl_agent_ppo' ê²½ë¡œì— ì €ì¥í•©ë‹ˆë‹¤.
"""

import os
import sys
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_dir, "../../../.."))
if project_root not in sys.path:
    sys.path.append(project_root)

from AI.modules.trader.train.rl_env import StockTradingEnv

def train_agent():
    print("ğŸš€ [RL] PPO íŠ¸ë ˆì´ë”© ì—ì´ì „íŠ¸ í•™ìŠµ ì‹œì‘")
    
    ticker = "AAPL" # í•™ìŠµí•  ì¢…ëª©
    start_date = "2020-01-01"
    end_date = "2023-12-31"
    
    # 1. í™˜ê²½ ìƒì„± (ë²¡í„°í™”ëœ í™˜ê²½)
    env = DummyVecEnv([lambda: StockTradingEnv(ticker, start_date, end_date)])
    
    # 2. ëª¨ë¸ ì„¤ì • (MlpPolicy: ìˆ˜ì¹˜ ë°ì´í„°ìš© ì‹ ê²½ë§)
    model = PPO(
        "MlpPolicy", 
        env, 
        verbose=1, 
        learning_rate=0.0003,
        batch_size=64,
        gamma=0.99, # ë¯¸ë˜ ë³´ìƒ í• ì¸ìœ¨
        ent_coef=0.01 # íƒí—˜ ì¥ë ¤
    )
    
    # 3. í•™ìŠµ (Timesteps: í•™ìŠµ íšŸìˆ˜)
    total_timesteps = 50_000
    print(f"   - í•™ìŠµ ê¸°ê°„: {start_date} ~ {end_date}")
    print(f"   - ì´ ìŠ¤í…: {total_timesteps}")
    
    model.learn(total_timesteps=total_timesteps)
    
    # 4. ì €ì¥
    save_dir = os.path.join(project_root, "AI", "data", "weights")
    os.makedirs(save_dir, exist_ok=True)
    save_path = os.path.join(save_dir, "rl_agent_ppo")
    
    model.save(save_path)
    print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}.zip")

if __name__ == "__main__":
    train_agent()