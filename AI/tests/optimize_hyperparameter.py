# AI/tests/optimization.py
"""
[í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (AutoML) - Stable & Strict Version]
- Optunaë¥¼ ì‚¬ìš©í•˜ì—¬ Transformer ëª¨ë¸ ìµœì í™”
- [ìˆ˜ì • ë‚´ì—­]:
  1. í”¼ì²˜ NaN ê²°ì¸¡ì¹˜ ì—„ê²©í•œ í•„í„°ë§
  2. Class Weight ê³„ì‚° ì‹œ int íƒ€ì… ë³´ì¥
  3. Validation ë°ì´í„° ì‹œí€€ìŠ¤ ìƒì„± ì‹œ Train í›„ë°˜ë¶€ Context í™œìš© (í‰ê°€ ë°ì´í„° ë³´ì¡´)
  4. ì ìˆ˜ ì‚°ì • ì‹œ Cooldown(ì¤‘ë³µ ì§„ì… ë°©ì§€) ì ìš©ìœ¼ë¡œ í˜„ì‹¤ì„± í™•ë³´
  5. ì˜ˆì™¸ ë°œìƒ ì‹œ ì•ˆì „í•œ ìì› í•´ì œ ë¡œì§
"""

import sys
import os
import gc
import optuna
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras import backend as K
from sklearn.preprocessing import MinMaxScaler

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  1. ê²½ë¡œ ë° ëª¨ë“ˆ ë¡œë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
sys.path.append(project_root)

from AI.modules.signal.core.data_loader import SignalDataLoader
from AI.modules.signal.models import get_model

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  2. ì „ì—­ ë°ì´í„° ë¡œë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("[AutoML] ë°ì´í„° ë©”ëª¨ë¦¬ ë¡œë”© ì¤‘...")

target_tickers = ["AAPL", "MSFT", "GOOGL", "AMZN", "NVDA", "TSLA", "META"] 
start_date = "2023-01-01"
end_date = "2024-12-31"

loader = SignalDataLoader(sequence_length=60)
grouped_data = []

for ticker in target_tickers:
    try:
        df = loader.load_data(ticker, start_date, end_date)
        if not df.empty and len(df) > 300:
            grouped_data.append(df)
    except Exception as e:
        print(f"[Warning] {ticker} ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

print(f"[AutoML] {len(grouped_data)}ê°œ ì¢…ëª© ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ.")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  3. Helper Functions
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _label_by_future_return(close_prices: pd.Series, horizon: int, threshold: float) -> tuple[pd.Series, pd.Series]:
    """
    [ìˆ˜ì • 6] ë¼ë²¨ íƒ€ì… ëª…ì‹œ (int32)
    """
    future_ret = (close_prices.shift(-horizon) / close_prices) - 1.0
    
    # 1=BUY, 0=HOLD
    labels = np.where(future_ret > threshold, 1, 0).astype(np.int32)
    
    # ë§ˆì§€ë§‰ horizon ê¸°ê°„ -1 ì²˜ë¦¬
    labels[-horizon:] = -1 
    
    return pd.Series(labels, index=close_prices.index), future_ret

def _build_sequences(data: np.ndarray, seq_len: int) -> np.ndarray:
    num_samples = len(data) - seq_len + 1
    if num_samples <= 0:
        return np.array([])
    
    X = []
    for i in range(num_samples):
        X.append(data[i : i+seq_len])
        
    return np.array(X)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  4. Objective Function
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def objective(trial):
    # (0) ë©”ëª¨ë¦¬ ì •ë¦¬
    K.clear_session()
    gc.collect()
    model_wrapper = None # [ìˆ˜ì • 3] ì´ˆê¸°í™”

    # (1) í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜
    SEQ_LEN = trial.suggest_categorical('seq_len', [30, 60, 90])
    PRED_H = 5
    HOLD_THR = trial.suggest_float('hold_thr', 0.01, 0.04) 
    
    LEARNING_RATE = trial.suggest_float('learning_rate', 1e-4, 5e-3, log=True)
    DROPOUT = trial.suggest_float('dropout', 0.1, 0.4)
    NUM_BLOCKS = trial.suggest_int('num_blocks', 1, 3)
    HEAD_SIZE = trial.suggest_categorical('head_size', [64, 128])
    NUM_HEADS = trial.suggest_categorical('num_heads', [2, 4])
    FF_DIM = trial.suggest_categorical('ff_dim', [64, 128, 256])
    
    PRED_THR = trial.suggest_float('pred_thr', 0.4, 0.7)

    # (2) ë°ì´í„°ì…‹ êµ¬ì„±
    if not grouped_data:
        return -999.0
    
    X_train_list, y_train_list = [], []
    X_val_list, y_val_list, r_val_list = [], [], []

    sample_df = grouped_data[0]
    feature_cols = sample_df.select_dtypes(include=[np.number]).columns.tolist()
    
    for df in grouped_data:
        labels, future_ret = _label_by_future_return(df["close"], PRED_H, HOLD_THR)
        
        # [ìˆ˜ì • 1] í”¼ì²˜ NaN í¬í•¨ ì—¬ë¶€ê¹Œì§€ ì—„ê²©í•˜ê²Œ ê²€ì‚¬
        valid_mask = (
            (labels != -1) & 
            future_ret.notna() & 
            df[feature_cols].notna().all(axis=1)
        )
        
        df_valid = df.loc[valid_mask]
        labels_valid = labels[valid_mask]
        ret_valid = future_ret[valid_mask]
        
        if len(df_valid) < SEQ_LEN + 20: # ì—¬ìœ ë¶„ í¬í•¨
            continue
            
        # Time Split (8:2)
        split_idx = int(len(df_valid) * 0.8)
        
        train_df = df_valid.iloc[:split_idx]
        val_df = df_valid.iloc[split_idx:]
        
        train_labels = labels_valid.iloc[:split_idx]
        val_labels = labels_valid.iloc[split_idx:]
        val_rets = ret_valid.iloc[split_idx:]
        
        # ìŠ¤ì¼€ì¼ë§
        scaler = MinMaxScaler()
        train_feat = train_df[feature_cols].values.astype(np.float32)
        scaler.fit(train_feat) # Train Fit
        
        # â”€â”€â”€ Train Data ìƒì„± â”€â”€â”€
        train_scaled = scaler.transform(train_feat)
        X_train_seq = _build_sequences(train_scaled, SEQ_LEN)
        y_train_seq = train_labels.values[SEQ_LEN-1:] # ì‹œí€€ìŠ¤ ëë‚˜ëŠ” ì‹œì ì˜ ë¼ë²¨
        
        min_len_train = min(len(X_train_seq), len(y_train_seq))
        if min_len_train > 0:
            X_train_list.append(X_train_seq[:min_len_train])
            y_train_list.append(y_train_seq[:min_len_train])
            
        # â”€â”€â”€ Val Data ìƒì„± (Context Prefix ì ìš©) [ìˆ˜ì • 4] â”€â”€â”€
        # ê²€ì¦ ë°ì´í„°ì˜ ì²« ë‚ ë¶€í„° ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ Trainì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ì„ ê°€ì ¸ì˜´
        lookback = SEQ_LEN - 1
        if len(train_df) >= lookback:
            # Train ë’·ë¶€ë¶„ + Val ì „ì²´
            val_input_df = pd.concat([train_df.iloc[-lookback:], val_df], axis=0)
            val_input_feat = val_input_df[feature_cols].values.astype(np.float32)
            val_scaled = scaler.transform(val_input_feat)
            
            X_val_seq = _build_sequences(val_scaled, SEQ_LEN)
            # Contextë¥¼ ë¶™ì˜€ìœ¼ë¯€ë¡œ, ìƒì„±ëœ ì‹œí€€ìŠ¤ ê°œìˆ˜ëŠ” ì •í™•íˆ val_df ê¸¸ì´ì™€ ê°™ìŒ
            # ë”°ë¼ì„œ slicing ë¶ˆí•„ìš” (ë‹¨, ê¸¸ì´ê°€ ë§ëŠ”ì§€ minìœ¼ë¡œ ì•ˆì „ì¥ì¹˜)
            
            y_val_seq = val_labels.values
            r_val_seq = val_rets.values
        else:
            # Trainì´ ë„ˆë¬´ ì§§ì€ ê²½ìš° (ì˜ˆì™¸ì ) -> ê¸°ì¡´ ë°©ì‹ fallback
            val_feat = val_df[feature_cols].values.astype(np.float32)
            val_scaled = scaler.transform(val_feat)
            X_val_seq = _build_sequences(val_scaled, SEQ_LEN)
            y_val_seq = val_labels.values[SEQ_LEN-1:]
            r_val_seq = val_rets.values[SEQ_LEN-1:]
        
        min_len_val = min(len(X_val_seq), len(y_val_seq), len(r_val_seq))
        if min_len_val > 0:
            X_val_list.append(X_val_seq[:min_len_val])
            y_val_list.append(y_val_seq[:min_len_val])
            r_val_list.append(r_val_seq[:min_len_val])

    if not X_train_list or not X_val_list:
        return -999.0

    # (3) ë°ì´í„° ë³‘í•© ë° ê°€ì¤‘ì¹˜ ê³„ì‚°
    X_train = np.concatenate(X_train_list, axis=0)
    # [ìˆ˜ì • 2] int32ë¡œ ëª…ì‹œì  ë³€í™˜
    y_train = np.concatenate(y_train_list, axis=0).astype(np.int32)
    
    X_val = np.concatenate(X_val_list, axis=0)
    y_val = np.concatenate(y_val_list, axis=0).astype(np.int32)
    r_val = np.concatenate(r_val_list, axis=0)
    
    classes = np.unique(y_train)
    class_weight_dict = None
    if len(classes) >= 2:
        weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)
        # [ìˆ˜ì • 2] Dict key: int, value: float ë³´ì¥
        class_weight_dict = {int(c): float(w) for c, w in zip(classes, weights)}
    
    # (4) ëª¨ë¸ í•™ìŠµ
    config = {
        "input_shape": (SEQ_LEN, len(feature_cols)),
        "head_size": HEAD_SIZE,
        "num_heads": NUM_HEADS,
        "ff_dim": FF_DIM,
        "num_blocks": NUM_BLOCKS,
        "mlp_units": [64], 
        "dropout": DROPOUT,
        "mlp_dropout": DROPOUT,
        "learning_rate": LEARNING_RATE
    }
    
    try:
        model_wrapper = get_model("transformer", config)
        model_wrapper.build(config["input_shape"])
        
        # model.train ë‚´ë¶€ì—ì„œ fit í˜¸ì¶œ ì‹œ y_trainì´ intì—¬ë„ binary_crossentropy(from_logits=False)ë©´ OK
        # ë‹¨, TransformerSignalModel êµ¬ì¡°ìƒ sigmoid ì¶œë ¥ì´ë©´ yëŠ” 0/1 (int or float) í˜¸í™˜ë¨
        model_wrapper.train(
            X_train, y_train,
            X_val=X_val, y_val=y_val,
            epochs=5,
            batch_size=1024,
            class_weight=class_weight_dict,
            callbacks=[EarlyStopping(patience=2, monitor='val_loss', restore_best_weights=True)],
            verbose=2
        )
        
        # (5) í‰ê°€ ë° ì ìˆ˜ ì‚°ì •
        y_pred_probs = model_wrapper.predict(X_val).flatten()
        buy_signals = (y_pred_probs > PRED_THR) # boolean mask
        
        if np.sum(buy_signals) < 5: # ìµœì†Œ ê±°ë˜ íšŸìˆ˜ ë¯¸ë‹¬
            return -10.0
            
        # [ìˆ˜ì • 5] Cooldown(ì¤‘ë³µ ì§„ì… ë°©ì§€) ì ìš© Score ê³„ì‚°
        # "BUY ì‹ í˜¸ê°€ ëœ¨ë©´ PRED_H(=5ì¼) ë™ì•ˆì€ ì¶”ê°€ ë§¤ìˆ˜ ë¶ˆê°€(ì´ë¯¸ í¬ì§€ì…˜ ë³´ìœ )" ê°€ì •
        
        total_profit = 0.0
        trade_count = 0
        last_exit_idx = -1  # ë§ˆì§€ë§‰ ë§¤ë„(ë³´ìœ  ì¢…ë£Œ) ì‹œì  ì¸ë±ìŠ¤
        
        # buy_signalsê°€ Trueì¸ ì¸ë±ìŠ¤ë§Œ ì¶”ì¶œ
        signal_indices = np.where(buy_signals)[0]
        
        for idx in signal_indices:
            # ì´ì „ ê±°ë˜ê°€ ëë‚œ ì´í›„ì—ë§Œ ì§„ì… ê°€ëŠ¥
            if idx > last_exit_idx:
                # ìˆ˜ìµ ì‹¤í˜„ (r_val[idx]ëŠ” idx ì‹œì  ë§¤ìˆ˜ í›„ 5ì¼ ë’¤ ìˆ˜ìµë¥ )
                total_profit += r_val[idx]
                trade_count += 1
                last_exit_idx = idx + PRED_H - 1 # ë³´ìœ  ê¸°ê°„ ì„¤ì •
        
        # ê±°ë˜ê°€ ë„ˆë¬´ ì ê²Œ ê±¸ëŸ¬ì¡Œì„ ê²½ìš° ì¬í™•ì¸
        if trade_count == 0:
            return -5.0

        # ìµœì¢… ì ìˆ˜: ëˆ„ì  ìˆ˜ìµë¥ 
        # (ì˜µì…˜) ìŠ¹ë¥ ì´ë‚˜ ê±°ë˜ íšŸìˆ˜ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ë¥¼ ë” ì¤„ ìˆ˜ë„ ìˆìŒ
        score = total_profit
        
        # ìŠ¹ë¥  ê³„ì‚° (ì‹¤ì œ ì§„ì…í•œ ê±°ë˜ ê¸°ì¤€)
        # ë£¨í”„ë¥¼ ë‹¤ì‹œ ëŒ í•„ìš” ì—†ì´, ìœ„ì—ì„œ ë”í•  ë•Œ ìŠ¹/íŒ¨ ì¹´ìš´íŒ… ê°€ëŠ¥í•˜ë‚˜ ê°„ëµí™”
        # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœ ëˆ„ì  ìˆ˜ìµì„ ìµœìš°ì„  ì§€í‘œë¡œ ì‚¼ìŒ
        
        return score

    except Exception as e:
        print(f"[Trial Fail] Error: {e}")
        # tracebackì„ ë³´ê³  ì‹¶ìœ¼ë©´ import traceback; traceback.print_exc() ì‚¬ìš©
        return -999.0
        
    finally:
        # [ìˆ˜ì • 3] ì•ˆì „í•œ ìì› í•´ì œ
        if model_wrapper is not None:
            del model_wrapper
        K.clear_session()
        gc.collect()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  5. ë©”ì¸ ì‹¤í–‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
        except RuntimeError as e:
            print(e)
            
    study = optuna.create_study(direction="maximize")
    
    print("ğŸš€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘ (Stable & Strict)")
    # ì˜ˆì™¸ë¡œ ì£½ì§€ ì•Šë„ë¡ catch_catch=True ì˜µì…˜ì„ ê³ ë ¤í•  ìˆ˜ ìˆìœ¼ë‚˜,
    # ì—¬ê¸°ì„œëŠ” objective ë‚´ë¶€ try-exceptë¡œ ì²˜ë¦¬í•¨.
    study.optimize(objective, n_trials=30, n_jobs=1)
    
    print("\n" + "="*50)
    print("ğŸ† Best Trial Result")
    print(f"  Score (Cooldown Total Return): {study.best_value:.4f}")
    print("  Best Params:")
    for key, value in study.best_params.items():
        print(f"    {key}: {value}")
    print("="*50)