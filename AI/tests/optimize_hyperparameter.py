# AI/tests/optimization.py
"""
[í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (AutoML)]
- Optunaë¥¼ ì‚¬ìš©í•˜ì—¬ Transformer ëª¨ë¸ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ë§¤ë§¤ ì„ê³„ê°’(Threshold)ì„ ìµœì í™”í•©ë‹ˆë‹¤.
- 'test' ì„±ê²©ì˜ ì‹¤í—˜ìš© ìŠ¤í¬ë¦½íŠ¸ì´ë¯€ë¡œ, íŒŒì´í”„ë¼ì¸ì—ëŠ” í¬í•¨ë˜ì§€ ì•Šê³  ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ ë°ì´í„° ë¡œë”©ì€ ì „ì—­ ë³€ìˆ˜ë¡œ ì²˜ë¦¬í•˜ì—¬ ì¤‘ë³µ ë¡œë“œë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
"""

import sys
import os
import argparse
import optuna
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras.callbacks import EarlyStopping

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  ê²½ë¡œ ë° ëª¨ë“ˆ ë¡œë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€ (ì ˆëŒ€ ê²½ë¡œ import ìœ„í•¨)
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_dir, "../../../.."))
if project_root not in sys.path:
    sys.path.append(project_root)

from AI.modules.signal.core.data_loader import SignalDataLoader
from AI.modules.signal.models import get_model
# (í•„ìš” ì‹œ) í‹°ì»¤ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ í•¨ìˆ˜ ì„í¬íŠ¸
# from AI.modules.finder.selector import load_all_tickers 

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  ì „ì—­ ë°ì´í„° ë¡œë“œ (Global Data Loading)
#  - Optuna Trialë§ˆë‹¤ DB ì ‘ì†ì„ ë°˜ë³µí•˜ì§€ ì•Šê¸° ìœ„í•´ í•œ ë²ˆë§Œ ë¡œë“œí•©ë‹ˆë‹¤.
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("[AutoML] ë°ì´í„° ë©”ëª¨ë¦¬ ë¡œë”© ì¤‘...")

# 1. ëŒ€ìƒ í‹°ì»¤ ì„ ì • (ì˜ˆì‹œ: S&P 500 ìƒìœ„ ì¢…ëª© ë“±)
# ì‹¤ì œë¡œëŠ” DBë‚˜ íŒŒì¼ì—ì„œ ê°€ì ¸ì˜¤ëŠ” ë¡œì§ ì‚¬ìš©
target_tickers = ["AAPL", "MSFT", "GOOGL", "AMZN", "NVDA", "TSLA", "META"] 

# 2. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ (í”¼ì²˜ ìƒì„±)
# í•™ìŠµ ê¸°ê°„ ì„¤ì •
start_date = "2023-01-01"
end_date = "2024-12-31"

loader = SignalDataLoader(sequence_length=60) # ê¸°ë³¸ ì‹œí€€ìŠ¤ ê¸¸ì´ëŠ” ë‚˜ì¤‘ì— ë³€ê²½ë¨
grouped_data = []

for ticker in target_tickers:
    try:
        # load_data ë‚´ë¶€ì—ì„œ fetch_ohlcv + add_technical_indicators ìˆ˜í–‰
        df = loader.load_data(ticker, start_date, end_date)
        if not df.empty and len(df) > 100:
            grouped_data.append(df)
    except Exception as e:
        print(f"[Warning] {ticker} ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

print(f"[AutoML] {len(grouped_data)}ê°œ ì¢…ëª© ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ.")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  Helper Functions (ë¼ë²¨ë§ ë° ì‹œí€€ìŠ¤ ìƒì„±)
#  - Trial ë‚´ë¶€ì—ì„œ ë™ì ìœ¼ë¡œ í˜¸ì¶œë©ë‹ˆë‹¤.
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _label_by_future_return(close_prices: pd.Series, horizon: int, threshold: float) -> pd.Series:
    """ë¯¸ë˜ ìˆ˜ìµë¥  ê¸°ë°˜ ë¼ë²¨ë§ (0: BUY, 1: HOLD)"""
    # future_return = (ë¯¸ë˜ ê°€ê²© / í˜„ì¬ ê°€ê²©) - 1
    future_ret = (close_prices.shift(-horizon) / close_prices) - 1.0
    
    # ì„ê³„ê°’ ì´ìƒì´ë©´ BUY(0), ì•„ë‹ˆë©´ HOLD(1)
    # (ì°¸ê³ : ì¼ë°˜ì ìœ¼ë¡œ 1ì´ Positive Classì§€ë§Œ, ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
    labels = np.where(future_ret > threshold, 0, 1)
    
    # ë§ˆì§€ë§‰ horizon ê¸°ê°„ì€ NaN ì²˜ë¦¬ (ë¯¸ë˜ ë°ì´í„° ì—†ìŒ)
    labels[-horizon:] = -1 
    return pd.Series(labels, index=close_prices.index), future_ret

def _build_sequences(df: pd.DataFrame, feature_cols: list, seq_len: int) -> np.ndarray:
    """ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±"""
    data = df[feature_cols].values
    num_samples = len(data) - seq_len + 1
    
    X = []
    for i in range(num_samples):
        X.append(data[i : i+seq_len])
        
    return np.array(X)

def _align_labels(labels: pd.Series, seq_len: int) -> np.ndarray:
    """ì‹œí€€ìŠ¤ì— ë§ì¶° ë¼ë²¨ ì •ë ¬"""
    # ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ ì‹œì ì— í•´ë‹¹í•˜ëŠ” ë¼ë²¨ì„ ê°€ì ¸ì˜´
    # ì…ë ¥ ì‹œí€€ìŠ¤: t ~ t+seq_len-1
    # ì˜ˆì¸¡ ëŒ€ìƒ: t+seq_len-1 ì‹œì ì—ì„œì˜ íŒë‹¨
    return labels.iloc[seq_len-1:].values


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  Objective Function (Optunaê°€ ìµœì í™”í•  ëŒ€ìƒ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def objective(trial):
    # 1. íƒìƒ‰í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜ (Search Space)
    SEQ_LEN = trial.suggest_categorical('seq_len', [30, 60, 128]) # ìœˆë„ìš° í¬ê¸°
    PRED_H = 7  # ì˜ˆì¸¡ ê¸°ê°„ì€ ê³ ì • (í•„ìš” ì‹œ ë³€ê²½ ê°€ëŠ¥)
    
    # â˜… í•µì‹¬: ë§¤ìˆ˜/ë§¤ë„ ê¸°ì¤€ ì„ê³„ê°’ë„ ìµœì í™” ëŒ€ìƒì— í¬í•¨
    HOLD_THR = trial.suggest_float('hold_thr', 0.002, 0.02) # 0.2% ~ 2.0% ì‚¬ì´ íƒìƒ‰
    
    # ëª¨ë¸ ê´€ë ¨ íŒŒë¼ë¯¸í„°
    LEARNING_RATE = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)
    DROPOUT = trial.suggest_float('dropout', 0.1, 0.5)
    NUM_LAYERS = trial.suggest_int('num_blocks', 1, 4) # layer -> blocks ìš©ì–´ í†µì¼
    HEAD_SIZE = trial.suggest_categorical('head_size', [64, 128, 256]) # d_model ëŒ€ì‹  head_size ì‚¬ìš©
    NUM_HEADS = trial.suggest_int('num_heads', 2, 8)
    FF_DIM = trial.suggest_int('ff_dim', 2, 8)
    
    # 2. ë°ì´í„°ì…‹ êµ¬ì„± (íŒŒë¼ë¯¸í„°ì— ë”°ë¼ ë¼ë²¨ì´ ë°”ë€Œë¯€ë¡œ ë§¤ë²ˆ ìƒì„±í•´ì•¼ í•¨)
    X_all, y_all, r_all = [], [], []
    
    # í”¼ì²˜ ì»¬ëŸ¼ ì„ íƒ (Date, Ticker ë“± ì œì™¸í•˜ê³  ìˆ˜ì¹˜í˜•ë§Œ)
    if not grouped_data:
        return -999.0
        
    sample_df = grouped_data[0]
    feature_cols = sample_df.select_dtypes(include=[np.number]).columns.tolist()
    
    # ë°ì´í„°ì…‹ ìƒì„± ë£¨í”„
    for df in grouped_data:
        # (1) ë¼ë²¨ë§ ë‹¤ì‹œ ìˆ˜í–‰ (HOLD_THRê°€ ë°”ë€Œë¯€ë¡œ)
        labels, future_ret = _label_by_future_return(df["close"], PRED_H, HOLD_THR)
        
        # (2) ìœ íš¨ ë°ì´í„° í•„í„°ë§
        # í”¼ì²˜, ë¼ë²¨, ë¯¸ë˜ìˆ˜ìµë¥  ëª¨ë‘ NaNì´ ì•„ë‹Œ êµ¬ê°„ë§Œ ì‚¬ìš©
        valid_mask = df[feature_cols].notna().all(axis=1) & (labels != -1) & future_ret.notna()
        
        df_valid = df[valid_mask]
        labels_valid = labels[valid_mask]
        ret_valid = future_ret[valid_mask]
        
        if len(df_valid) < SEQ_LEN:
            continue
            
        # (3) ìŠ¤ì¼€ì¼ë§ (ê°„ì†Œí™”ë¥¼ ìœ„í•´ ê° ì¢…ëª©ë³„ MinMax ì ìš©)
        # ì£¼ì˜: ì—„ë°€í•œ ê²€ì¦ì„ ìœ„í•´ì„œëŠ” Train/Val ë¶„ë¦¬ í›„ ìŠ¤ì¼€ì¼ë§í•´ì•¼ í•¨
        from sklearn.preprocessing import MinMaxScaler
        scaler = MinMaxScaler()
        scaled_features = scaler.fit_transform(df_valid[feature_cols])
        df_scaled = pd.DataFrame(scaled_features, columns=feature_cols, index=df_valid.index)
        
        # (4) ì‹œí€€ìŠ¤ ìƒì„±
        X_seq = _build_sequences(df_scaled, feature_cols, SEQ_LEN)
        y_seq = _align_labels(labels_valid, SEQ_LEN)
        r_seq = _align_labels(ret_valid, SEQ_LEN) # ìˆ˜ìµë¥  ì‹œí€€ìŠ¤
        
        # ê¸¸ì´ ë§ì¶¤
        min_len = min(len(X_seq), len(y_seq), len(r_seq))
        X_all.append(X_seq[:min_len])
        y_all.append(y_seq[:min_len])
        r_all.append(r_seq[:min_len])

    if not X_all:
        return -999.0 # ì‹¤íŒ¨ ì‹œ ë§¤ìš° ë‚®ì€ ì ìˆ˜

    X = np.concatenate(X_all, axis=0)
    y = np.concatenate(y_all, axis=0).astype(int)
    r = np.concatenate(r_all, axis=0)
    
    # Train/Val ë¶„ë¦¬ (ìµœê·¼ 20%ë¥¼ ê²€ì¦ìš©ìœ¼ë¡œ ì‚¬ìš© - ì‹œê³„ì—´ ê³ ë ¤)
    # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœ Shuffle ì—†ì´ ë’¤ìª½ ë°ì´í„°ë¥¼ Valë¡œ ì‚¬ìš©
    split_idx = int(len(X) * 0.8)
    X_train, X_val = X[:split_idx], X[split_idx:]
    y_train, y_val = y[:split_idx], y[split_idx:]
    r_val = r[split_idx:] # ê²€ì¦ ë°ì´í„°ì˜ ì‹¤ì œ ìˆ˜ìµë¥ 
    
    # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (HOLD í¸í–¥ ë°©ì§€)
    classes = np.unique(y_train)
    if len(classes) < 2:
        return -999.0 # í´ë˜ìŠ¤ê°€ í•˜ë‚˜ë¿ì´ë©´ í•™ìŠµ ë¶ˆê°€
        
    weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)
    class_weight_dict = {i: w for i, w in zip(classes, weights)}

    # 3. ëª¨ë¸ ë¹Œë“œ ë° í•™ìŠµ
    # get_model íŒ©í† ë¦¬ í•¨ìˆ˜ ì‚¬ìš©
    config = {
        "input_shape": (SEQ_LEN, X.shape[2]),
        "head_size": HEAD_SIZE,
        "num_heads": NUM_HEADS,
        "ff_dim": FF_DIM,
        "num_blocks": NUM_LAYERS,
        "mlp_units": [128], # ê³ ì •ê°’ ë˜ëŠ” íƒìƒ‰ ê°€ëŠ¥
        "dropout": DROPOUT,
        "mlp_dropout": DROPOUT,
        "learning_rate": LEARNING_RATE
    }
    
    model_wrapper = get_model("transformer", config)
    model_wrapper.build(config["input_shape"])
    
    # ì†ë„ë¥¼ ìœ„í•´ EpochëŠ” ì§§ê²Œ ì„¤ì • (Pruning í™œìš© ê°€ëŠ¥)
    # fit() ë©”ì„œë“œ ì§ì ‘ í˜¸ì¶œ ëŒ€ì‹  wrapperì˜ train ì‚¬ìš©
    history = model_wrapper.train(
        X_train, y_train,
        X_val=X_val, y_val=y_val,
        epochs=5,          # ìµœì í™” ë‹¨ê³„ì—ì„œëŠ” epochë¥¼ ì¤„ì„
        batch_size=512,
        class_weight=class_weight_dict,
        callbacks=[EarlyStopping(patience=2, monitor='val_loss')]
    )
    
    # 4. ë°±í…ŒìŠ¤íŒ… ì‹œë®¬ë ˆì´ì…˜ (ê²€ì¦ ë°ì´í„°ì…‹ ëŒ€ìƒ)
    # wrapperì˜ predict ë©”ì„œë“œ ì‚¬ìš© (í™•ë¥ ê°’ ë°˜í™˜)
    # TransformerSignalModelì€ ê¸°ë³¸ì ìœ¼ë¡œ ì´ì§„ ë¶„ë¥˜(sigmoid) -> 0.5 ê¸°ì¤€
    # í•˜ì§€ë§Œ ìœ„ ë¡œì§ì€ Sparse Categorical (ë‹¤ì¤‘ë¶„ë¥˜) ë¡œì§ì„ ë”°ë¥´ê³  ìˆìœ¼ë¯€ë¡œ,
    # ëª¨ë¸ ì•„í‚¤í…ì²˜ê°€ ì´ì§„ë¶„ë¥˜(sigmoid)ì¸ì§€ ë‹¤ì¤‘ë¶„ë¥˜(softmax)ì¸ì§€ì— ë”°ë¼ ì²˜ë¦¬ê°€ ë‹¤ë¦…ë‹ˆë‹¤.
    # í˜„ì¬ architecture.pyëŠ” sigmoid(ì´ì§„ë¶„ë¥˜)ë¡œ ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì´ì— ë§ê²Œ ì¡°ì •í•©ë‹ˆë‹¤.
    
    y_pred_probs = model_wrapper.predict(X_val) # (N, 1) í˜•íƒœ
    
    # 0: BUY, 1: HOLD ë¼ê³  ê°€ì •í–ˆìœ¼ë‚˜, 
    # Sigmoid ì¶œë ¥ 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ "ìƒìŠ¹(Positive)" -> BUY
    # Sigmoid ì¶œë ¥ 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ "í•˜ë½/ë³´í•©(Negative)" -> HOLD
    # ë”°ë¼ì„œ, score > 0.5 ì´ë©´ BUYë¡œ ê°„ì£¼
    
    buy_signals = (y_pred_probs.flatten() > 0.5)
    
    if np.sum(buy_signals) == 0:
        return 0.0 # ë§¤ìˆ˜ ì‹ í˜¸ê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ 0ì  ì²˜ë¦¬
        
    # í‰ê°€ì§€í‘œ: 'BUY ì‹ í˜¸ ì‹œ í‰ê·  ìˆ˜ìµë¥  * ì ì¤‘ë¥ '
    # r_valì€ ì‹¤ì œ ìˆ˜ìµë¥ 
    
    avg_return = np.mean(r_val[buy_signals])
    win_rate = np.mean(r_val[buy_signals] > 0)
    
    # ì ìˆ˜ ì‚°ì • ê³µì‹ (ì‚¬ìš©ì ì •ì˜ ê°€ëŠ¥)
    # ì˜ˆ: í‰ê·  ìˆ˜ìµë¥ ì´ ë†’ìœ¼ë©´ì„œë„, ë„ˆë¬´ ì ê²Œ ê±°ë˜í•˜ì§€ ì•ŠëŠ” ê· í˜•ì  ì°¾ê¸°
    score = avg_return * 100 
    
    # ë„ˆë¬´ ìœ„í—˜í•œ ë§¤ë§¤ë¥¼ ë§‰ê¸° ìœ„í•´ ìŠ¹ë¥  í˜ë„í‹° ì¶”ê°€ ê°€ëŠ¥
    if win_rate < 0.5: 
        score *= 0.5 
        
    return score

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  ì‹¤í–‰ (Main)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    # ë°©í–¥: maximize (ìˆ˜ìµë¥  ì ìˆ˜ ìµœëŒ€í™”)
    study = optuna.create_study(direction="maximize")
    
    print("ğŸš€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘ (ì´ 20íšŒ ì‹œë„)")
    # n_jobs=1 (TensorFlow ì¶©ëŒ ë°©ì§€)
    study.optimize(objective, n_trials=20, n_jobs=1)
    
    print("\n" + "="*50)
    print("ğŸ† Best Trial ê²°ê³¼:")
    print(f"  Value (Score): {study.best_value}")
    print("  Params:")
    for key, value in study.best_params.items():
        print(f"    {key}: {value}")
    print("="*50)
    
    # ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥ (ì„ íƒ ì‚¬í•­)
    # import json
    # with open('best_params.json', 'w') as f:
    #     json.dump(study.best_params, f)