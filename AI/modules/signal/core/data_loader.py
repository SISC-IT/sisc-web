# AI/modules/signal/core/data_loader.py
"""
[Data Loader - Integrated & Dynamic Version]
- ì£¼ê°€(Price), ê±°ì‹œê²½ì œ(Macro), ì‹œì¥ì§€í‘œ(Breadth), ë‰´ìŠ¤ì‹¬ë¦¬(Sentiment), í€ë”ë©˜í„¸(Fundamental) ë°ì´í„°ë¥¼ í†µí•© ë¡œë“œí•©ë‹ˆë‹¤.
- í…Œì´ë¸”ë³„ë¡œ ë°ì´í„°ë¥¼ ì¡°íšŒí•œ ë’¤, Pandas Mergeë¥¼ í†µí•´ ì‹œê³„ì—´ì„ ì •ë ¬í•©ë‹ˆë‹¤.
- Multi-Horizon (ì˜ˆ: 1, 3, 5, 7ì¼) ì˜ˆì¸¡ì„ ë™ì ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ë¼ë²¨ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import sys
import os
import numpy as np
import pandas as pd
from typing import Tuple, Dict, List, Optional
from sklearn.preprocessing import MinMaxScaler
from sqlalchemy import text
from tqdm import tqdm

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_dir, "../../../.."))
if project_root not in sys.path:
    sys.path.append(project_root)

# DB ì—°ê²° ë° Fetcher ëª¨ë“ˆ import
from AI.libs.database.connection import get_engine
from AI.libs.database.fetcher import (
    fetch_macro_indicators, 
    fetch_market_breadth, 
    fetch_news_sentiment, 
    fetch_fundamentals
)
from AI.modules.signal.core.features import add_technical_indicators

class DataLoader:
    def __init__(self, db_name="db", lookback=60, horizons: List[int] = None):
        """
        :param db_name: DB ì—°ê²° ì„¤ì • ì´ë¦„
        :param lookback: ì‹œí€€ìŠ¤ ê¸¸ì´ (ê³¼ê±° ë©°ì¹ ì„ ë³¼ ê²ƒì¸ê°€)
        :param horizons: ì˜ˆì¸¡í•  ë¯¸ë˜ ì‹œì  ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: [1, 3, 5] -> 1ì¼ë’¤, 3ì¼ë’¤, 5ì¼ë’¤ ì˜ˆì¸¡)
                         Noneì¼ ê²½ìš° ê¸°ë³¸ê°’ [1, 3, 5, 7] ì‚¬ìš©
        """
        self.db_name = db_name
        self.lookback = lookback
        self.horizons = horizons if horizons else [1, 3, 5, 7]
        self.scaler = MinMaxScaler()
        
        # ë©”íƒ€ë°ì´í„° ID ë§¤í•‘
        self.ticker_to_id: Dict[str, int] = {}
        self.sector_to_id: Dict[str, int] = {}
        self.ticker_sector_map: Dict[str, int] = {}
        
        # ê³µí†µ ë°ì´í„° ìºì‹± (Macro, Market Breadth)
        self.macro_df: pd.DataFrame = pd.DataFrame()
        self.breadth_df: pd.DataFrame = pd.DataFrame()
        
        # ì´ˆê¸°í™” ì‹œ ë©”íƒ€ë°ì´í„° ë¡œë“œ
        self._load_metadata()

    def _load_metadata(self):
        """ì¢…ëª© ë° ì„¹í„° ì •ë³´ë¥¼ ë¡œë“œí•˜ì—¬ ID ë§¤í•‘ ìƒì„±"""
        engine = get_engine(self.db_name)
        try:
            query = text("SELECT ticker, COALESCE(sector, 'Unknown') as sector FROM public.stock_info")
            with engine.connect() as conn:
                df_meta = pd.read_sql(query, conn)
            
            if df_meta.empty:
                print("[DataLoader] Warning: stock_info í…Œì´ë¸”ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return

            unique_sectors = sorted(df_meta['sector'].unique().tolist())
            self.sector_to_id = {sec: i for i, sec in enumerate(unique_sectors)}
            
            for _, row in df_meta.iterrows():
                self.ticker_sector_map[row['ticker']] = self.sector_to_id[row['sector']]
                
            self.ticker_to_id = {t: i for i, t in enumerate(df_meta['ticker'])}
            print(f"[DataLoader] ë©”íƒ€ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.ticker_to_id)}ê°œ ì¢…ëª©")
            
        except Exception as e:
            print(f"[DataLoader] ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

    def _prepare_common_data(self, start_date: str):
        """
        [ìµœì í™”] ëª¨ë“  ì¢…ëª©ì— ê³µí†µìœ¼ë¡œ ì ìš©ë˜ëŠ” ê±°ì‹œê²½ì œ/ì‹œì¥ì§€í‘œë¥¼ ë¯¸ë¦¬ í•œ ë²ˆë§Œ ë¡œë“œí•©ë‹ˆë‹¤.
        """
        try:
            print("[DataLoader] ê³µí†µ ë°ì´í„°(Macro, Breadth) ë¡œë“œ ì¤‘...")
            self.macro_df = fetch_macro_indicators(start_date, self.db_name)
            self.breadth_df = fetch_market_breadth(start_date, self.db_name)
        except Exception as e:
            print(f"[DataLoader] ê³µí†µ ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ë¬´ì‹œí•˜ê³  ì§„í–‰): {e}")

    def load_data_from_db(self, start_date="2018-01-01") -> pd.DataFrame:
        """
        1. ê³µí†µ ë°ì´í„°ë¥¼ ë¨¼ì € ë¡œë“œí•©ë‹ˆë‹¤.
        2. ì „ì²´ ì¢…ëª©ì˜ ì£¼ê°€ ë°ì´í„°(Price Data)ë¥¼ ëŒ€ëŸ‰ìœ¼ë¡œ ì¡°íšŒí•©ë‹ˆë‹¤.
        """
        # 1. ê³µí†µ ë°ì´í„° ì¤€ë¹„
        self._prepare_common_data(start_date)
        
        # 2. ì£¼ê°€ ë°ì´í„° Bulk Load
        print(f"[DataLoader] {start_date} ë¶€í„° ì „ì²´ ì£¼ê°€ ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤...")
        engine = get_engine(self.db_name)
        
        # ê±°ë˜ëŒ€ê¸ˆ(amount) í¬í•¨
        query = text("""
            SELECT date, ticker, open, high, low, close, volume, adjusted_close, amount
            FROM public.price_data
            WHERE date >= :start_date
            ORDER BY ticker, date ASC
        """)
        
        with engine.connect() as conn:
            df_price = pd.read_sql(query, conn, params={"start_date": start_date})
            
        if not df_price.empty:
            df_price['date'] = pd.to_datetime(df_price['date'])
            # ìˆ˜ì •ì£¼ê°€(adjusted_close) ìš°ì„  ì‚¬ìš©
            if 'adjusted_close' in df_price.columns:
                df_price['close'] = df_price['adjusted_close'].fillna(df_price['close'])
        
        print(f"[DataLoader] ì£¼ê°€ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df_price)} rows")
        return df_price

    def create_dataset(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, Dict]:
        """
        ë¡œë“œëœ ì£¼ê°€ ë°ì´í„°(df)ë¥¼ ìˆœíšŒí•˜ë©°:
        1. ê³µí†µ ë°ì´í„°(Macro, Breadth) ë³‘í•©
        2. ê°œë³„ ë°ì´í„°(News, Fundamental) ì¡°íšŒ ë° ë³‘í•©
        3. ê¸°ìˆ ì  ì§€í‘œ ìƒì„± ë° ìŠ¤ì¼€ì¼ë§
        4. ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹ ìƒì„± (X, y)
        """
        X_ts_list, X_ticker_list, X_sector_list = [], [], []
        y_class_list, y_reg_list = [], []
        
        # ë™ì ìœ¼ë¡œ ì„¤ì •ëœ í˜¸ë¼ì´ì¦Œ ì‚¬ìš©
        max_horizon = max(self.horizons)
        
        tickers = df['ticker'].unique()
        print(f"ğŸ¯ ì˜ˆì¸¡ ëª©í‘œ: {self.horizons}ì¼ ë’¤ ë“±ë½ ë™ì‹œ ì˜ˆì¸¡ (Max Horizon: {max_horizon}ì¼)")
        
        processed_dfs = []
        
        # -------------------------------------------------------------------------
        # Step 1: ì¢…ëª©ë³„ ë°ì´í„° ë³‘í•© ë° ì „ì²˜ë¦¬ (Merging & Feature Engineering)
        # -------------------------------------------------------------------------
        for ticker in tqdm(tickers, desc="Processing Tickers"):
            # í•´ë‹¹ ì¢…ëª© ë°ì´í„° ì¶”ì¶œ
            sub_df = df[df['ticker'] == ticker].copy().sort_values('date')
            
            # ë°ì´í„° ìµœì†Œ ê¸¸ì´ í™•ì¸ (lookback + max_horizon ë³´ë‹¤ ì‘ìœ¼ë©´ ì‹œí€€ìŠ¤ ìƒì„± ë¶ˆê°€)
            if len(sub_df) <= self.lookback + max_horizon: continue
            if sub_df['close'].std() == 0: continue # ë³€ë™ì„± ì—†ëŠ” ë°ì´í„° ì œì™¸

            # [Merge 1] ê³µí†µ ë°ì´í„° ë³‘í•© (Left Join)
            if not self.macro_df.empty:
                sub_df = pd.merge(sub_df, self.macro_df, on='date', how='left')
            if not self.breadth_df.empty:
                sub_df = pd.merge(sub_df, self.breadth_df, on='date', how='left')
            
            # [Merge 2] ê°œë³„ ë°ì´í„° ì¡°íšŒ ë° ë³‘í•© (News, Fundamentals)
            try:
                # 2-1. ë‰´ìŠ¤ ì‹¬ë¦¬ (ì¢…ëª©ë³„)
                df_news = fetch_news_sentiment(ticker, sub_df['date'].min().strftime('%Y-%m-%d'), self.db_name)
                if not df_news.empty:
                    sub_df = pd.merge(sub_df, df_news, on='date', how='left')
                    sub_df[['sentiment_score', 'risk_keyword_cnt']] = sub_df[['sentiment_score', 'risk_keyword_cnt']].fillna(0)
                
                # 2-2. í€ë”ë©˜í„¸ (ì¢…ëª©ë³„) - ffill ì‚¬ìš©
                df_fund = fetch_fundamentals(ticker, self.db_name)
                if not df_fund.empty:
                    sub_df = pd.merge(sub_df, df_fund, on='date', how='left')
                    fund_cols = ['per', 'pbr', 'roe', 'debt_ratio']
                    cols_to_fill = [c for c in fund_cols if c in sub_df.columns]
                    sub_df[cols_to_fill] = sub_df[cols_to_fill].ffill().fillna(0)
            except Exception:
                pass # ë¶€ê°€ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œí•˜ê³  ì§„í–‰

            # [Preprocessing] ê²°ì¸¡ì¹˜ ë³´ê°„ (Macro ì£¼ë§ ë°ì´í„° ë“±)
            sub_df = sub_df.ffill().bfill()

            # [Feature Engineering] ê¸°ìˆ ì  ì§€í‘œ ìƒì„±
            try:
                sub_df = add_technical_indicators(sub_df)
            except Exception:
                continue
            
            processed_dfs.append(sub_df)
            
        if not processed_dfs: 
            raise ValueError("[Error] ì „ì²˜ë¦¬ëœ ìœ íš¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
        full_df = pd.concat(processed_dfs)
        full_df['raw_close'] = full_df['close'] # ìŠ¤ì¼€ì¼ë§ ì „ ì›ë³¸ ê°€ê²© ë³´ì¡´
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ Feature ìë™ ê°ì§€
        potential_features = [
            # 1. Technical
            'log_return', 'open_ratio', 'high_ratio', 'low_ratio', 'vol_change',
            'ma_5_ratio', 'ma_20_ratio', 'ma_60_ratio', 
            'rsi', 'macd_ratio', 'bb_position',
            # 2. Macro (New)
            'us10y', 'yield_spread', 'vix_close', 'dxy_close', 'credit_spread_hy',
            # 3. Breadth (New)
            'advance_decline_ratio', 'fear_greed_index',
            # 4. Sentiment (New)
            'sentiment_score', 'risk_keyword_cnt',
            # 5. Fundamental (New)
            'per', 'pbr', 'roe'
        ]
        
        available_cols = [c for c in potential_features if c in full_df.columns]
        full_df = full_df.dropna(subset=available_cols)
        
        print(f">> Scaling Features: {len(available_cols)} columns selected")
        print(f"   (Included: {available_cols})")
        
        # Scaling (ì „ì²´ ë°ì´í„° ê¸°ì¤€ fitting)
        full_df[available_cols] = self.scaler.fit_transform(full_df[available_cols])

        # -------------------------------------------------------------------------
        # Step 2: ì‹œí€€ìŠ¤ ìƒì„± (Sequencing)
        # -------------------------------------------------------------------------
        print(">> Generating Sequences & Labels...")
        
        # ì†ë„ ìµœì í™”ë¥¼ ìœ„í•´ numpy ë³€í™˜ í›„ ë£¨í”„ ìˆ˜í–‰
        # (ì¢…ëª©ë³„ë¡œ groupí•˜ì—¬ ì²˜ë¦¬)
        for ticker in tqdm(full_df['ticker'].unique(), desc="Sequencing"):
            sub_df = full_df[full_df['ticker'] == ticker]
            
            # ì‹œí€€ìŠ¤ ìƒì„± ê°€ëŠ¥ ê¸¸ì´ í™•ì¸ (ìœ„ì—ì„œ í–ˆì§€ë§Œ dropna ë“±ìœ¼ë¡œ ì¤„ì–´ë“¤ì—ˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¬í™•ì¸)
            if len(sub_df) <= self.lookback + max_horizon: continue

            # ë©”íƒ€ë°ì´í„° ë§¤í•‘
            t_id = self.ticker_to_id.get(ticker, 0)
            s_id = self.ticker_sector_map.get(ticker, 0)

            # Numpy ë³€í™˜ (ì†ë„ ìµœì í™”)
            feature_vals = sub_df[available_cols].values
            raw_closes = sub_df['raw_close'].values
            
            # ë£¨í”„ ë²”ìœ„ ê³„ì‚°: ë§ˆì§€ë§‰ ë°ì´í„°ì—ì„œ max_horizon ë§Œí¼ì€ ì •ë‹µì„ ì•Œ ìˆ˜ ì—†ìŒ
            num_samples = len(sub_df) - self.lookback - max_horizon + 1
            if num_samples <= 0: continue

            for i in range(num_samples):
                # X: ê³¼ê±° ë°ì´í„° Window (Sequence)
                window = feature_vals[i : i + self.lookback]
                
                # y: ë¯¸ë˜ ì˜ˆì¸¡ (Multi-Horizon)
                curr_price = raw_closes[i + self.lookback - 1]
                
                multi_labels = []
                # ë™ì  horizons ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¼ë²¨ ìƒì„±
                for h in self.horizons:
                    future_price = raw_closes[i + self.lookback + h - 1]
                    # ë“±ë½ ë¼ë²¨ (1: ìƒìŠ¹, 0: í•˜ë½/ë³´í•©)
                    label = 1 if future_price > curr_price else 0
                    multi_labels.append(label)
                
                # Regression Target (ì˜ˆì‹œ: ê°€ì¥ ë¨¼ ë¯¸ë˜ ìˆ˜ìµë¥ )
                label_reg = 0.0
                if curr_price != 0:
                    label_reg = (raw_closes[i + self.lookback + max_horizon - 1] - curr_price) / curr_price

                X_ts_list.append(window)
                X_ticker_list.append(t_id)
                X_sector_list.append(s_id)
                y_class_list.append(multi_labels)
                y_reg_list.append(label_reg)

        # ê²°ê³¼ ë³€í™˜
        X_ts = np.array(X_ts_list)
        X_ticker = np.array(X_ticker_list)
        X_sector = np.array(X_sector_list)
        y_class = np.array(y_class_list) # Shape: (N, len(horizons))
        y_reg = np.array(y_reg_list)
        
        info = {
            "n_tickers": len(self.ticker_to_id),
            "n_sectors": len(self.sector_to_id),
            "feature_names": available_cols,
            "n_features": len(available_cols),
            "horizons": self.horizons, # ë©”íƒ€ë°ì´í„°ì— ì‚¬ìš©ëœ horizons ê¸°ë¡
            "scaler": self.scaler
        }
        
        print(f"[Dataset Ready] Samples: {len(y_class)}, Features: {len(available_cols)}")
        return X_ts, X_ticker, X_sector, y_class, y_reg, info