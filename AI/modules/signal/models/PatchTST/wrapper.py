# AI/modules/signal/models/PatchTST/wrapper.py
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import os
from typing import Optional, Dict, Any
from ...core.base_model import BaseSignalModel
from .architecture import PatchTST_Model

class PatchTSTWrapper(BaseSignalModel):
    """
    [PatchTST êµ¬í˜„ì²´] BaseSignalModel ì¸í„°í˜ì´ìŠ¤ ì¤€ìˆ˜
    - ìš©ë„: ì¤‘ì¥ê¸° ì¶”ì„¸ ì˜ˆì¸¡ (Trend Specialist)
    """
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None # build() í˜¸ì¶œ ì‹œ ìƒì„±ë¨

    def build(self, input_shape: tuple):
        """
        ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„±
        Args:
            input_shape: (seq_len, num_features) ì˜ˆ: (120, 7)
        """
        seq_len, num_features = input_shape
        
        self.model = PatchTST_Model(
            seq_len=seq_len,
            enc_in=num_features,
            patch_len=self.config.get('patch_len', 16),
            stride=self.config.get('stride', 8),
            d_model=self.config.get('d_model', 128),
            dropout=self.config.get('dropout', 0.1)
        ).to(self.device)
        print(f"âœ… PatchTST Built: Input {input_shape} -> Output [1] (Prob)")

    def train(self, X_train: np.ndarray, y_train: np.ndarray, 
              X_val: Optional[np.ndarray] = None, y_val: Optional[np.ndarray] = None, **kwargs):
        """ëª¨ë¸ í•™ìŠµ ìˆ˜í–‰ (BCE Loss)"""
        if self.model is None:
            self.build(X_train.shape[1:]) # (Seq, Feat)

        criterion = nn.BCEWithLogitsLoss()
        optimizer = optim.AdamW(self.model.parameters(), lr=self.config.get('lr', 0.0001))
        epochs = self.config.get('epochs', 50)
        batch_size = self.config.get('batch_size', 32)

        # Tensor ë³€í™˜
        X_tensor = torch.FloatTensor(X_train).to(self.device)
        y_tensor = torch.FloatTensor(y_train).view(-1, 1).to(self.device)

        self.model.train()
        for epoch in range(epochs):
            # (ê°„ì†Œí™”ë¥¼ ìœ„í•´ ë°°ì¹˜ ë£¨í”„ ìƒëµí•˜ê³  ì „ì²´ ì£¼ì… ì˜ˆì‹œ - ì‹¤ì œë¡  DataLoader ì‚¬ìš© ê¶Œì¥)
            permutation = torch.randperm(X_tensor.size(0))
            epoch_loss = 0
            
            for i in range(0, X_tensor.size(0), batch_size):
                indices = permutation[i:i+batch_size]
                batch_x, batch_y = X_tensor[indices], y_tensor[indices]

                optimizer.zero_grad()
                output = self.model(batch_x)
                loss = criterion(output, batch_y)
                loss.backward()
                optimizer.step()
                epoch_loss += loss.item()
            
            if (epoch + 1) % 10 == 0:
                print(f"Epoch [{epoch+1}/{epochs}] Loss: {epoch_loss:.4f}")

    def predict(self, X_input: np.ndarray) -> np.ndarray:
        """ì¶”ë¡  ìˆ˜í–‰: ìƒìŠ¹ í™•ë¥ (0~1) ë°˜í™˜"""
        if self.model is None:
            raise Exception("Model not initialized. Call build() or load() first.")
            
        self.model.eval()
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X_input).to(self.device)
            logits = self.model(X_tensor)
            probs = torch.sigmoid(logits).cpu().numpy() # Logit -> Probability
            
        return probs

    def save(self, filepath: str):
        torch.save(self.model.state_dict(), filepath)
        print(f"ğŸ’¾ PatchTST saved to {filepath}")

    def load(self, filepath: str):
        # ë¡œë“œ ì‹œì—ëŠ” configì— ìˆëŠ” shape ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ buildë¥¼ ë¨¼ì € í•´ì•¼ í•¨
        # (í˜¹ì€ ì €ì¥ ì‹œ shape ì •ë³´ë¥¼ ê°™ì´ ì €ì¥í•˜ëŠ” ë°©ì‹ ì‚¬ìš©)
        if self.model is None:
             # ì„ì‹œ: configì— ì €ì¥ëœ shape ì‚¬ìš© (ìš´ì˜ ì‹œ ë³´ì™„ í•„ìš”)
            self.build((self.config.get('seq_len', 120), self.config.get('enc_in', 7)))
            
        self.model.load_state_dict(torch.load(filepath, map_location=self.device))
        self.model.eval()
        print(f"ğŸ“‚ PatchTST loaded from {filepath}")