import os
import sys
import pickle
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau

# --------------------------------------------------------------------------
# 1. GPU ë° í™˜ê²½ ì„¤ì •
# --------------------------------------------------------------------------
print("í…ì„œí”Œë¡œìš° ë²„ì „:", tf.__version__)
print("GPU ëª©ë¡:", tf.config.list_physical_devices('GPU'))
print("\n" + "="*50)

# OOM(ë©”ëª¨ë¦¬ ë¶€ì¡±) ë°©ì§€ë¥¼ ìœ„í•œ GPU ë©”ëª¨ë¦¬ ì ì§„ì  í• ë‹¹ ì„¤ì •
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print(f"ğŸš€ GPU ë°œê²¬ë¨! ({len(gpus)}ëŒ€): {gpus}")
    except RuntimeError as e:
        print(e)
else:
    print("ğŸ¢ GPU ì—†ìŒ... CPU ì‚¬ìš©í•©ë‹ˆë‹¤.")
print("="*50 + "\n")

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_dir, "../../../.."))
if project_root not in sys.path:
    sys.path.append(project_root)

from AI.modules.signal.core.data_loader import DataLoader
from AI.modules.signal.models.PatchTST.architecture import build_transformer_model

def train_single_pipeline():
    print("==================================================")
    print(" [Training] Multi-Horizon Model (1, 3, 5, 7 Days)")
    print("==================================================")

    # --------------------------------------------------------------------------
    # 2. ë°ì´í„° ë¡œë“œ ë° ê¸°ê°„ ë¶„ë¦¬ (Data Leakage ë°©ì§€)
    # --------------------------------------------------------------------------
    loader = DataLoader(lookback=60)
    print(">> ë°ì´í„° ë¡œë”© ë° ì§€í‘œ ìƒì„± ì¤‘...")
    
    # ì „ì²´ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¨ ë’¤...
    full_df = loader.load_data_from_db(start_date="2015-01-01")
    
    # [í•µì‹¬] 2023ë…„ 12ì›” 31ì¼ ì´ì „ ë°ì´í„°ë§Œ í•™ìŠµì— ì‚¬ìš©! (ë¯¸ë˜ ë°ì´í„° ì°¨ë‹¨)
    raw_df = full_df[full_df['date'] <= '2023-12-31'].copy()
    
    print(f">> í•™ìŠµ ë°ì´í„° ê¸°ê°„: {raw_df['date'].min()} ~ {raw_df['date'].max()}")
    print(f">> ì´ ë°ì´í„° í–‰ ìˆ˜: {len(raw_df)} rows")

    # --------------------------------------------------------------------------
    # 3. ë°ì´í„°ì…‹ ìƒì„± (Sequencing)
    # --------------------------------------------------------------------------
    # y_classëŠ” (N, 4) í˜•íƒœ: [1ì¼ë’¤, 3ì¼ë’¤, 5ì¼ë’¤, 7ì¼ë’¤]
    X_ts, X_ticker, X_sector, y_class, _, info = loader.create_dataset(raw_df)
    
    # [ë””ë²„ê·¸] ì •ë‹µ ë¶„í¬ í™•ì¸
    horizons = info.get("horizons", [1])
    n_outputs = len(horizons) # ë³´í†µ 4ê°œ
    
    print("\n" + "="*50)
    print(f" ğŸš¨ [DEBUG] Multi-Horizon ë°ì´í„° ì ê²€ ({horizons}ì¼)")
    print("="*50)
    for i, h in enumerate(horizons):
        col_data = y_class[:, i]
        unique, counts = np.unique(col_data, return_counts=True)
        dist = dict(zip(unique, counts))
        ratio = counts[1] / sum(counts) * 100 if 1 in dist else 0
        print(f" - [{h}ì¼ ë’¤] ìƒìŠ¹ ë¹„ìœ¨: {ratio:.2f}% (ë¶„í¬: {dist})")
    print("="*50 + "\n")

    # --------------------------------------------------------------------------
    # 4. ë°ì´í„° ë¶„í•  (Train / Validation)
    # --------------------------------------------------------------------------
    X_ts_train, X_ts_val, \
    X_tick_train, X_tick_val, \
    X_sec_train, X_sec_val, \
    y_train, y_val = train_test_split(
        X_ts, X_ticker, X_sector, y_class,
        test_size=0.2, shuffle=True, random_state=42
    )
    
    # --------------------------------------------------------------------------
    # 5. ëª¨ë¸ ë¹Œë“œ
    # --------------------------------------------------------------------------
    print(f">> ëª¨ë¸ ë¹Œë“œ ì¤‘ (Outputs: {n_outputs})...")
    
    model = build_transformer_model(
        input_shape=(X_ts.shape[1], X_ts.shape[2]),
        n_tickers=info['n_tickers'],
        n_sectors=info['n_sectors'],
        n_outputs=n_outputs # 4ê°œ ì¶œë ¥
    )
    
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)
    
    model.compile(
        optimizer=optimizer,
        loss="binary_crossentropy", 
        metrics=["accuracy"]
    )

    # --------------------------------------------------------------------------
    # 6. ì½œë°± ì„¤ì • (í•™ìŠµ ì „ëµì˜ í•µì‹¬)
    # --------------------------------------------------------------------------
    save_dir = os.path.join(project_root, "AI/data/weights/transformer")
    os.makedirs(save_dir, exist_ok=True)
    model_save_path = os.path.join(save_dir, "multi_horizon_model.keras")

    # (1) ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ (ì „ì„±ê¸° ìº¡ì²˜)
    chk_point = ModelCheckpoint(
        filepath=model_save_path,
        monitor='val_loss',
        save_best_only=True, 
        verbose=1
    )
    
    # (2) ì¡°ê¸° ì¢…ë£Œ (10ë²ˆ ì°¸ì•˜ëŠ”ë° ì•ˆ ì¢‹ì•„ì§€ë©´ ë©ˆì¶¤)
    early_stop = EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True
    )

    # (3) í•™ìŠµë¥  ì¡°ì • (5ë²ˆ ì°¸ì•˜ëŠ”ë° ì•ˆ ì¢‹ì•„ì§€ë©´ ë” ì„¸ë°€í•˜ê²Œ í•™ìŠµ)
    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=5,
        min_lr=1e-6,
        verbose=1
    )

    # --------------------------------------------------------------------------
    # 7. í•™ìŠµ ì‹œì‘
    # --------------------------------------------------------------------------
    print(">> í•™ìŠµ ì‹œì‘ (Epochs=50)...")
    model.fit(
        x=[X_ts_train, X_tick_train, X_sec_train],
        y=y_train,
        validation_data=([X_ts_val, X_tick_val, X_sec_val], y_val),
        epochs=50,
        batch_size=32, # [ì¤‘ìš”] OOM ë°©ì§€ë¥¼ ìœ„í•´ 32ë¡œ ì„¤ì •
        shuffle=True,
        callbacks=[chk_point, early_stop, reduce_lr] # ì½œë°± ì ìš©
    )
    
    # --------------------------------------------------------------------------
    # 8. ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ (í•„ìˆ˜)
    # --------------------------------------------------------------------------
    # ëª¨ë¸ì€ chk_pointê°€ ì´ë¯¸ ì €ì¥í–ˆìœ¼ë¯€ë¡œ, ìŠ¤ì¼€ì¼ëŸ¬ë§Œ ë”°ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    scaler_save_path = os.path.join(save_dir, "multi_horizon_scaler.pkl")
    with open(scaler_save_path, "wb") as f:
        pickle.dump(info['scaler'], f)
        
    print(f"\n[ì™„ë£Œ] í•™ìŠµ ì¢…ë£Œ. ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f" - ëª¨ë¸ ê²½ë¡œ: {model_save_path}")
    print(f" - ìŠ¤ì¼€ì¼ëŸ¬: {scaler_save_path}")

if __name__ == "__main__":
    train_single_pipeline()